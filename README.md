Análisis comparativo de la detección de errores en textos escolares por chatbots de IA generativa y docentes expertos
Comparative Analysis of Error Detection in School Texts by Generative AI Chatbots and Expert Teachers

Resumen
Este estudio compara la eficacia de distintos chatbots basados en inteligencia artificial generativa (IAG) —ChatGPT, Claude, DeepSeek, Mistral y Gemini— en la detección de errores lingüísticos en textos de aprendientes de español como lengua extranjera (ELE). El objetivo principal es determinar qué modelo se aproxima con mayor precisión a la corrección experta, considerada como gold standard. Para ello, se emplea un corpus de 116 producciones escritas auténticas de niveles A2–B1, pertenecientes a tipologías expositivas, descriptivas e instructivas, frente a los cuales se contrastan las detecciones de cada sistema tras la aplicación de una cadena de prompts.
El análisis se centra en el número total de errores identificados correctamente, sin distinguir su tipología, con el fin de evitar sesgos tipológicos en esta fase exploratoria y valorar la eficacia global de los modelos. Se aplican métricas consolidadas en evaluación automática —precisión, recall y F1-score—, junto con un análisis de falsos positivos por cada mil tokens, a fin de obtener una medida más ajustada a la variabilidad y densidad de errores característica de los textos de aprendientes.
Los resultados permiten establecer un ranking comparativo de los modelos, examinar su rendimiento según el nivel MCER de los textos y analizar la relación entre el nivel asignado por los evaluadores humanos y el estimado por los modelos de IAG. El estudio ofrece una visión crítica y cuantitativa sobre el potencial y las limitaciones de estas herramientas, aportando evidencias empíricas para orientar su uso educativo en contextos de corrección asistida y enseñanza de lenguas mediada por IAG.
Palabras clave: Inteligencia Artificial Generativa; enseñanza de español; detección de errores; evaluación automática; Procesamiento del Lenguaje Natural.

Abstract
This study compares the effectiveness of several generative artificial intelligence (GenAI) chatbots—ChatGPT, Claude, DeepSeek, Mistral, and Gemini—in detecting linguistic errors in texts written by learners of Spanish as a foreign language (SFL). The main objective is to determine which model most closely approximates expert correction, considered the gold standard. To this end, a corpus of 116 authentic written productions at A2–B1 levels, covering expository, descriptive, and instructive text types, is used to contrast the detections produced by each system following the application of a prompt chain.
The analysis focuses on the total number of correctly identified errors, without distinguishing their typology, in order to avoid typological bias in this exploratory phase and to assess the overall effectiveness of the models. Standard metrics in automatic evaluation—precision, recall, and F1-score—are applied, along with an analysis of false positives per thousand tokens, to obtain a measure that better captures the variability and error density characteristic of learner texts.
The results make it possible to establish a comparative ranking of the models, examine their performance according to the CEFR level of the texts, and analyze the relationship between the level assigned by human evaluators and that estimated by the GenAI systems. The study offers a critical and quantitative perspective on the potential and limitations of these tools, providing empirical evidence to inform their educational use in contexts of assisted correction and language teaching supported by GenAI.
Keywords: Generative Artificial Intelligence; Spanish language teaching; error detection; automatic evaluation; Natural Language Processing.
